{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing & Iteration\n",
    "\n",
    "The development of the AI-driven personal assistant involves a rigorous process of testing and iteration to ensure functionality and reliability.\n",
    "\n",
    "## Initial Implementation: Speech Recognition from Audio File\n",
    "\n",
    "Initially, I implemented basic functionality to transcribe audio to text. This served as the foundation for speech recognition. The code here allowed recognition of speech from a pre-recorded audio file.\n",
    "\n",
    "*Comment: Code to recognize speech from an audio file.*\n",
    "**Code**:\n",
    "```python\n",
    "def recognize_from_audio(audio_file_path):\n",
    "    # Set up the speech configuration with your subscription info and region\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=API_KEY, region=REGION)\n",
    "    \n",
    "    # Specify the audio file to recognize\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=audio_file_path)\n",
    "\n",
    "    # Create a speech recognizer using the given settings\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Start speech recognition and get the result\n",
    "    result = recognizer.recognize_once()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(f\"Recognized: {result.text}\")\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "# Replace with the path to the audio file you want to transcribe\n",
    "audio_file_path = \"KSA2.wav\"\n",
    "```\n",
    "\n",
    "\n",
    "## Enhancing Real-Time Voice Input\n",
    "\n",
    "Following the initial success, I modified the implementation to support real-time voice input via the microphone. This enhancement allowed dynamic interaction between users and the assistant.\n",
    "\n",
    "\n",
    "\n",
    "*Comment: Code to enable real-time speech recognition via microphone.*\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def recognize_speech_continuously():\n",
    "    # Create a speech configuration with your subscription info and region\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=API_KEY, region=REGION)\n",
    "    \n",
    "    # Create a speech recognizer using the default microphone\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Listening... Speak into your microphone.\")\n",
    "\n",
    "    # Start continuous recognition\n",
    "    def recognized_handler(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(f\"Recognized: {evt.result.text}\")\n",
    "            # Here you can process the recognized text (e.g., send it to your agent for a response)\n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized.\")\n",
    "        elif evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "            print(f\"Recognition canceled: {evt.result.cancellation_details.reason}\")\n",
    "\n",
    "    # Use the `add_event` method to register the event handler\n",
    "    recognizer.recognized.connect(recognized_handler)\n",
    "\n",
    "    # Start the continuous recognition\n",
    "    recognizer.start_continuous_recognition()\n",
    "\n",
    "    # Keep the application running until you stop it (e.g., use a keyboard interrupt)\n",
    "    try:\n",
    "        while True:\n",
    "            pass  # Keep the script running\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping recognition...\")\n",
    "        recognizer.stop_continuous_recognition()\n",
    "```\n",
    "\n",
    "## Managing Two-Way Conversation\n",
    "\n",
    "To enable effective two-way communication, I introduced a `call_state` variable to manage the assistant's current state, either \"LISTENING\" or \"SPEAKING.\" This addition controlled when the assistant listened for user input and when it generated responses. \n",
    "\n",
    "\n",
    "\n",
    "*Comment: Code to manage assistant's state for two-way conversation .*\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "\n",
    "\n",
    "def recognize_speech_continuously():\n",
    "    global call_state,recognizer\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=API_KEY, region=REGION)\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    \n",
    "    \n",
    "    print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    initiate_conversation_with_llm()\n",
    "    print(\"Listening... Speak into your microphone.\")\n",
    "\n",
    "    def recognized_handler(evt):\n",
    "        global call_state,recognizer\n",
    "        if call_state != \"LISTENING\":\n",
    "            print(\"[DEBUG] ASR ignored input since call_state is not LISTENING.\")\n",
    "            return\n",
    "\n",
    "        # Process recognized speech\n",
    "        print(\"LISTINGIGN AGAIN\")\n",
    "        print(f\"speechsdk.ResultReason.RecognizedSpeech :{speechsdk.ResultReason.RecognizedSpeech}\")\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            recognized_text = evt.result.text.strip()\n",
    "            if recognized_text:\n",
    "                print(f\"[DEBUG] ASR recognized text: {recognized_text}\")\n",
    "                call_state = \"SPEAKING\"  # Switch to speaking state\n",
    "                recognizer.stop_continuous_recognition()  # Stop ASR during response processing\n",
    "                print(\"[DEBUG] ASR stopped for LLM response processing.\")\n",
    "                \n",
    "                # Process the recognized text with process_chunk\n",
    "                llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred = process_chunk(recognized_text)\n",
    "                \n",
    "                # Handle the LLM response\n",
    "                if llm_response:\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "                elif function_name:\n",
    "                    messages.append(\n",
    "                            {\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"content\": None,\n",
    "                                \"tool_calls\": [\n",
    "                                    {\n",
    "                                        \"id\": function_id,\n",
    "                                        \"function\": {\"name\": function_name, \"arguments\": function_arguments},\n",
    "                                        \"type\": \"function\",\n",
    "                                    }\n",
    "                                ],\n",
    "                            }\n",
    "                        )\n",
    "                    print(f\"[DEBUG] Calling function '{function_name}' with arguments: {function_arguments}\")\n",
    "                                # Call the WhatsApp sending function\n",
    "                    function_returns = send_message_to_whatsapp(function_arguments)\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"tool_call_id\": function_id,\n",
    "                            \"role\": \"tool\",\n",
    "                            \"name\": function_name,\n",
    "                            \"content\": function_returns,\n",
    "                        }\n",
    "                    )\n",
    "                    llm_response, _, _, _, _, _ = process_chunk(None)\n",
    "                    print(f\"LLM Response after funtion call:- {llm_response}\")\n",
    "                    messages.append({\"role\":\"assistant\",\"content\":llm_response})\n",
    "                    call_state = \"LISTENING\"\n",
    "                recognizer.start_continuous_recognition()    \n",
    "        elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"[DEBUG] No speech could be recognized.\")\n",
    "        elif evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "            print(f\"[DEBUG] Recognition canceled: {evt.result.cancellation_details.reason}\")\n",
    "\n",
    "    recognizer.recognized.connect(recognized_handler)\n",
    "    # recognizer.start_continuous_recognition()\n",
    "    # Start ASR only if it's in the correct state\n",
    "    if call_state == \"LISTENING\":\n",
    "        recognizer.start_continuous_recognition()\n",
    "        print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            if call_state == \"STOP\":\n",
    "                recognizer.stop_continuous_recognition()\n",
    "                print(\"[DEBUG] ASR now is STOPPOING\")\n",
    "                break \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"traceback in recognize_speech_continuously : {traceback.format_exc()}\")\n",
    "        recognizer.stop_continuous_recognition()\n",
    "```\n",
    "## Iterative Refinement\n",
    "\n",
    "This is the final refined code for the Automatic Speech Recognition (ASR) system, which enhances readability and passes the recognized text to the Language Model (LLM) for further processing.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def initialize_speech_recognizer():\n",
    "    \"\"\"Initialize the Azure speech recognizer.\"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_API_KEY, region=AZURE_REGION)\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    return recognizer\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def start_recognition_if_listening(recognizer):\n",
    "    \"\"\"Start continuous recognition if the call state is LISTENING.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    if call_state == \"LISTENING\":\n",
    "        recognizer.start_continuous_recognition()\n",
    "        print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ASR is not in the LISTENING state.\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def process_recognized_text(recognized_text, recognizer):\n",
    "    \"\"\"Process the recognized text and interact with the LLM.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    change_call_state(\"SPEAKING\")\n",
    "    recognizer.stop_continuous_recognition()  # Stop ASR during response processing\n",
    "    print(\"[DEBUG] ASR stopped for LLM response processing.\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def log_llm_response(llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred):\n",
    "    \"\"\"Log details of the LLM response.\"\"\"\n",
    "    print(f\"[DEBUG] LLM Response: {llm_response}\")\n",
    "    print(f\"[DEBUG] Function Name: {function_name}\")\n",
    "    print(f\"[DEBUG] Function Arguments: {function_arguments}\")\n",
    "    print(f\"[DEBUG] Function ID: {function_id}\")\n",
    "    print(f\"[DEBUG] First Chunk Time: {first_chunk_time} ms\")\n",
    "    print(f\"[DEBUG] Error Occurred: {error_occurred}\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def handle_llm_response(llm_response, function_name, function_arguments, function_id):\n",
    "    \"\"\"Handle the LLM response appropriately.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    if llm_response:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "    elif function_name:\n",
    "        append_asst_msg(messages=messages, function_id=function_id, function_name=function_name,\n",
    "                        function_args=function_arguments)\n",
    "        print(f\"[DEBUG] Calling function '{function_name}' with arguments: {function_arguments}\")\n",
    "        # Call the WhatsApp sending function\n",
    "        function_returns = send_message_to_whatsapp(function_arguments)\n",
    "        append_tool_call_message(messages=messages, function_id=function_id, function_name=function_name,\n",
    "                                 function_returns=function_returns)\n",
    "        llm_response, _, _, _, _, _ = process_chunk(None, client, messages, tools)\n",
    "        print(f\"LLM Response after function call: {llm_response}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "    \n",
    "    change_call_state(\"LISTENING\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def handle_recognition_result(evt, recognizer):\n",
    "    \"\"\"Handle the recognition result from the speech recognizer.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "\n",
    "    if call_state != \"LISTENING\":\n",
    "        print(\"[DEBUG] ASR ignored input since call_state is not LISTENING.\")\n",
    "        return\n",
    "\n",
    "    print(\"[DEBUG] LISTENING AGAIN\")\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        recognized_text = evt.result.text.strip()\n",
    "        if recognized_text:\n",
    "            print(f\"[DEBUG] ASR recognized text: {recognized_text}\")\n",
    "            process_recognized_text(recognized_text, recognizer)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"[DEBUG] No speech could be recognized.\")\n",
    "    elif evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "        print(f\"[DEBUG] Recognition canceled: {evt.result.cancellation_details.reason}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Process the recognized text with process_chunk\n",
    "    llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred = process_chunk(recognized_text, client, messages, tools)\n",
    "    \n",
    "    # Log the results\n",
    "    log_llm_response(llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred)\n",
    "\n",
    "    # Handle the LLM response\n",
    "    handle_llm_response(llm_response, function_name, function_arguments, function_id)\n",
    "    recognizer.start_continuous_recognition()\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def recognize_speech_continuously():\n",
    "    \"\"\"Continuously recognize speech using Azure Speech Service.\"\"\" \n",
    "    call_state = get_call_state()  \n",
    "    \n",
    "    recognizer = initialize_speech_recognizer()\n",
    "    change_recognizer(recognizer)  # Ensure the recognizer is set globally\n",
    "    print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    print(\"[DEBUG] ASR initialized. Waiting to start recognition...\")\n",
    "    \n",
    "    initiate_conversation_with_llm()\n",
    "    print(\"Listening... Speak into your microphone.\")\n",
    "    \n",
    "    recognizer.recognized.connect(lambda evt: handle_recognition_result(evt, recognizer))\n",
    "    \n",
    "    # Start ASR only if it's in the correct state\n",
    "    start_recognition_if_listening(recognizer)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            call_state = get_call_state()\n",
    "            if call_state == \"STOP\":\n",
    "                recognizer.stop_continuous_recognition()\n",
    "                print(\"[DEBUG] ASR is now STOPPING\")\n",
    "                break \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[DEBUG] Stopping recognition...\")\n",
    "        print(f\"traceback in recognize_speech_continuously: {traceback.format_exc()}\")\n",
    "        recognizer.stop_continuous_recognition()\n",
    "\n",
    "```\n",
    "\n",
    "**Through this iterative process of coding, testing, and refining, I was able to enhance the Azure ASR's capabilities, ensuring a smooth and engaging user experience. Each modification was guided by testing outcomes, allowing me to build a robust solution that effectively meets user needs.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of System Prompt through Testing and Iteration\n",
    "\n",
    "## 1. Initial Version (Basic Level)\n",
    "This is a simple outline without much detail.\n",
    "\n",
    "```plaintext\n",
    "Your role is to be a personal assistant for Mr. Ravi Ranjan. Manage his calls when he is not available.\n",
    "\n",
    "1. Greet the caller.\n",
    "2. Ask for their name and message.\n",
    "3. Summarize the message.\n",
    "4. Send the summary to Mr. Ravi Ranjan.\n",
    "5. Be professional and polite.\n",
    "```\n",
    "\n",
    "## Sample Outputs\n",
    "**Caller**: Hi, is Mr. Ravi Ranjan available?  \n",
    "**AI Assistant**: Hello, I’m the personal assistant for Mr. Ravi Ranjan. He is currently unavailable.  \n",
    "\n",
    "**Caller**: I’m Rahul. I need to discuss an urgent business matter.  \n",
    "**AI Assistant**: Got it, Rahul. I’ll relay that message to Mr. Ranjan.\n",
    "\n",
    "## 2. Enhanced Version\n",
    "Added more detail and clarified tasks. \n",
    "\n",
    "```plaintext\n",
    "You are Mr. Ravi Ranjan’s personal assistant. Your job is to handle calls when he can't.\n",
    "\n",
    "1. Greet the caller and let them know he is unavailable.\n",
    "2. Ask for the caller’s name and the reason for their call.\n",
    "3. Encourage them to provide details about their message.\n",
    "4. Summarize what they say, capturing important points.\n",
    "5. Confirm the message with the caller before sending it.\n",
    "6. Send the summary to Mr. Ravi Ranjan via WhatsApp.\n",
    "7. Maintain a professional and respectful tone.\n",
    "\n",
    "```\n",
    "## Sample Outputs\n",
    "**Caller**: Hi, is Mr. Ravi Ranjan available?  \n",
    "**AI Assistant**: Hello! I’m Mr. Ravi Ranjan’s personal assistant. He’s currently unavailable.  \n",
    "\n",
    "**Caller**: I’m Rahul. I need to discuss an urgent business matter.  \n",
    "**AI Assistant**: Thank you, Rahul. Could you please explain the reason for your call and share any important details?  \n",
    "\n",
    "**Caller**: I need to finalize the contract by the end of the week.  \n",
    "**AI Assistant**: Thank you for sharing, Rahul. To confirm, you mentioned that you need to finalize the contract by the end of the week. Is that correct?  \n",
    "\n",
    "**Caller**: Yes, that’s correct.  \n",
    "**AI Assistant**: Great! I’ll make sure Mr. Ranjan receives this message.\n",
    "\n",
    "\n",
    "## 3. Structured Version\n",
    "Introduced clearer structure and formatting for readability.\n",
    "\n",
    "```plaintext\n",
    "As Mr. Ravi Ranjan’s personal assistant, you will manage his calls. Your tasks include:\n",
    "\n",
    "1. **Greeting the Caller**: Inform them that Mr. Ranjan is unavailable.\n",
    "2. **Capturing Information**:\n",
    "   - Request the caller's name and reason for their call.\n",
    "   - Encourage detailed messages (e.g., “Please provide any important details.”).\n",
    "3. **Message Confirmation**:\n",
    "   - Summarize the caller’s message.\n",
    "   - Confirm the summary with the caller.\n",
    "4. **Sending the Summary**: \n",
    "   - Relay the message to Mr. Ranjan via WhatsApp.\n",
    "5. **Professional Tone**: \n",
    "   - Always be respectful and professional.\n",
    "\n",
    "```\n",
    "\n",
    "## Sample Outputs\n",
    "**Caller**: Hi, is Mr. Ravi Ranjan available?  \n",
    "**AI Assistant**: Hello, I’m LinkDesk, Mr. Ravi Ranjan’s personal assistant. He is currently unavailable.  \n",
    "\n",
    "**Caller**: I’m Rahul. I need to discuss an urgent business matter.  \n",
    "**AI Assistant**: Thank you, Rahul. Could you please provide me with the details of your message?  \n",
    "\n",
    "**Caller**: I need to finalize the contract by the end of the week.  \n",
    "**AI Assistant**: Thank you for that information. To confirm, you mentioned needing to finalize the contract by the end of the week. Is that correct?  \n",
    "\n",
    "**Caller**: Yes, that’s right.  \n",
    "**AI Assistant**: Thank you! I will send this summary to Mr. Ranjan now.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Detailed Version\n",
    "Added specific language for guidance and behavior.\n",
    "\n",
    "```plaintext\n",
    "You are LinkDesk, the smart personal assistant for Mr. Ravi Ranjan. Your mission is to manage his calls when he is unavailable:\n",
    "\n",
    "1. **Greeting the Caller**: \n",
    "   - Introduce yourself as Mr. Ranjan’s personal assistant and inform the caller of his unavailability.\n",
    "\n",
    "2. **Capturing the Caller’s Message**:\n",
    "   - Ask for their name and the purpose of their call.\n",
    "   - Encourage detailed messages (e.g., “Could you please explain the reason for your call and share any important details?”).\n",
    "   - Accurately summarize key points and urgent matters.\n",
    "\n",
    "3. **Confirming the Message**:\n",
    "   - Confirm the captured message (e.g., “Thank you for sharing. To confirm, you mentioned [insert summary]. Is that correct?”).\n",
    "\n",
    "4. **Sending the Summary**:\n",
    "   - Prepare a clear summary of the caller’s message and send it to Mr. Ranjan via WhatsApp.\n",
    "   - Inform the caller that their message has been recorded.\n",
    "\n",
    "5. **Maintaining Professionalism**: \n",
    "   - Use a professional and humble tone, ensuring the caller feels valued.\n",
    "\n",
    "```\n",
    "\n",
    "## Sample Outputs\n",
    "**Caller**: Hi, is Mr. Ravi Ranjan available?  \n",
    "**AI Assistant**: Hello! I’m LinkDesk, Mr. Ravi Ranjan’s personal assistant. He is currently unavailable.  \n",
    "\n",
    "**Caller**: I’m Rahul. I need to discuss an urgent business matter.  \n",
    "**AI Assistant**: Thank you, Rahul. May I ask for your name and the purpose of your call?  \n",
    "\n",
    "**Caller**: My name is Rahul, and I need to finalize the contract by the end of the week.  \n",
    "**AI Assistant**: Thank you for sharing, Rahul. To confirm, you mentioned that you need to finalize the contract by the end of the week. Is that correct?  \n",
    "\n",
    "**Caller**: Yes, that’s correct.  \n",
    "**AI Assistant**: I will prepare a summary of your message and send it to Mr. Ranjan via WhatsApp. Thank you for your patience.\n",
    "\n",
    "\n",
    "## 5.Better Version\n",
    "all refinements and the final structure\n",
    "\n",
    "```plaintext\n",
    "Your role is to serve as LinkDesk, the smart personal assistant for {recipient_name}, responsible for managing his calls when he's unavailable. Here's your mission:\n",
    "\n",
    "1. **Greeting the Caller**: Introduce yourself as {recipient_name}’s  personal assistant and inform the caller that he is currently unavailable.\n",
    "\n",
    "2. **Capturing the Caller’s Message**:\n",
    "   - Ask the caller for their name and the purpose of their call.\n",
    "   - Encourage them to share a detailed message, guiding them if needed (e.g., “Could you please explain the reason for your call and share any important details?”).\n",
    "   - Summarize the caller’s message, ensuring key points and any urgent matters are noted accurately.\n",
    "\n",
    "3. **Confirming the Message**:\n",
    "   - After the caller finishes, confirm the captured message. For example, say, “Thank you for sharing. To confirm, you mentioned [insert summary]. Is that correct?”\n",
    "\n",
    "4. **Sending the Summary**:\n",
    "   - Prepare a concise and clear summary of the caller’s message, including their name and purpose of the call.\n",
    "   - Use the send_message_to_whatsapp tool to relay this summary promptly to {recipient_name}  via his preferred communication channel (e.g., WhatsApp).\n",
    "   - Inform the caller politely that their message has been recorded and shared with {recipient_name}.\n",
    "\n",
    "5. **Maintaining Professionalism with Humility**:\n",
    "   - Use a professional yet humble tone in all interactions, reflecting positively on {recipient_name}.\n",
    "   - Ensure the caller feels respected and valued, regardless of the nature of their inquiry.\n",
    "\n",
    "Your goal is to ensure seamless communication between callers and {recipient_name}, capturing important messages accurately and delivering them efficiently.\n",
    "```\n",
    "\n",
    "## Sample Outputs\n",
    "**Caller**: Hi, is Mr. Ravi Ranjan available?  \n",
    "**AI Assistant**: Hello! I’m LinkDesk, Mr. Ravi Ranjan’s personal assistant. He is currently unavailable.  \n",
    "\n",
    "**Caller**: I’m Rahul. I need to discuss an urgent business matter.  \n",
    "**AI Assistant**: Thank you, Rahul. Could you please explain the reason for your call and share any important details?  \n",
    "\n",
    "**Caller**: I need to finalize the contract by the end of the week.  \n",
    "**AI Assistant**: Thank you for sharing. To confirm, you mentioned that you need to finalize the contract by the end of the week. Is that correct?  \n",
    "\n",
    "**Caller**: Yes, that’s right.  \n",
    "**AI Assistant**: Great! I will prepare a concise summary of your message, including your name and the purpose of your call. I’ll send this to Mr. Ranjan via WhatsApp right away. Your message has been recorded and will be shared with him.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Iteration for the LLM Module\n",
    "\n",
    "\n",
    "\n",
    "## Initial Implementation\n",
    "Initially, I implemented a function to process user requests and interact with the LLM. The goal was to handle basic user inputs and provide corresponding responses efficiently.\n",
    "\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def process_chunk(chunk=None):\n",
    "    if chunk is None:\n",
    "        print(\"[ERROR] Empty chunk!\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ]\n",
    "        )\n",
    "        description = response.choices[0].message.content\n",
    "        print(\"[DEBUG] Processed chunk successfully.\")  \n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while processing the chunk: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "```\n",
    "**Observation:**  \n",
    "The initial implementation worked well with smaller inputs. However, processing larger requests led to significant increases in response times.\n",
    "\n",
    "---\n",
    "\n",
    "## Enhancing Functionality with Tools\n",
    "To improve the function's capabilities, I added support for external tools. This allowed the LLM to utilize additional resources for generating responses.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def process_chunk(chunk=None):\n",
    "    if chunk is None:\n",
    "        print(\"[ERROR] Empty chunk!\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ],\n",
    "            tools=tools\n",
    "        )\n",
    "        description = response.choices[0].message.content\n",
    "        print(\"[DEBUG] Processed chunk successfully.\")  \n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while processing the chunk: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "```\n",
    "\n",
    "**Testing Outcome:**  \n",
    "Despite the enhancement, response times for larger inputs were still slower than desired. This prompted further exploration to optimize the performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Streaming\n",
    "To address performance concerns, I introduced streaming. This modification allowed the function to receive responses incrementally, improving delivery time for the initial chunks.\n",
    "\n",
    "\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def process_chunk(request):\n",
    "    try:\n",
    "        # Append user message to the messages array\n",
    "        if request:\n",
    "            messages.append({\"role\": \"user\", \"content\": request})\n",
    "\n",
    "        # Create the chat completion request with streaming\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        # Process the streaming response\n",
    "        for chunk in response:\n",
    "            print(f\"chunk: {chunk}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occur :{e}\") \n",
    "\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "chunk: ChatCompletionChunk(id='chatcmpl-AebdfzrVJ6aGlaEgPDkjdTyyGuZAG', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1734241167, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_6fc10e10eb', usage=None)\n",
    "chunk: ChatCompletionChunk(id='chatcmpl-AebdfzrVJ6aGlaEgPDkjdTyyGuZAG', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1734241167, model='gpt-4o-mini-2024-07-18', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_6fc10e10eb', usage=None)\n",
    "```\n",
    "\n",
    "**Observations:**  \n",
    "Incremental responses were successfully received, but the processing logic needed refinement to manage these chunks effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Refining Chunk Processing Logic\n",
    "I developed logic to extract and process streamed chunks, implementing punctuation-based splitting to construct coherent outputs.\n",
    "\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def process_chunk(request):\n",
    "    try:\n",
    "        # Append user message to the messages array\n",
    "        if request:\n",
    "            messages.append({\"role\": \"user\", \"content\": request})\n",
    "\n",
    "        # Create the chat completion request with streaming\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        # Initialize variables for processing the response\n",
    "        complete_string = \"\"\n",
    "        continious_string = \"\"\n",
    "        chunk_timestamp = time.time()\n",
    "\n",
    "        # Process the streaming response\n",
    "        for chunk in response:\n",
    "            print(f\"chunk: {chunk}\")\n",
    "            current_gpt_chunk = chunk.choices[0].delta.content if chunk.choices else \"\"\n",
    "\n",
    "            if current_gpt_chunk and chunk.choices and chunk.choices[0].delta:\n",
    "                # Determine break punctuation\n",
    "                break_punctuation = [',', '!', '.', '?']\n",
    "                \n",
    "                if any(punc in current_gpt_chunk for punc in break_punctuation):\n",
    "                    continious_string += current_gpt_chunk\n",
    "                    complete_string += current_gpt_chunk\n",
    "\n",
    "                    # Print the chunk for debugging\n",
    "                    print(\"chunk:->\", continious_string)\n",
    "                    \n",
    "                    continious_string = \"\"\n",
    "                else:\n",
    "                    continious_string += current_gpt_chunk\n",
    "\n",
    "        return complete_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while processing the chunk: {e}\")\n",
    "        return None\n",
    "\n",
    "```\n",
    "\n",
    "**Key Changes:**\n",
    "1. Introduced variables to manage ongoing and complete strings.\n",
    "2. Used break punctuation (e.g., `,`, `.`, `!`, `?`) to split and assemble meaningful chunks.\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "Enter your query: hi how you can help me\n",
    "[DEBUG] Time taken to generate 1st chunk: 28.403043746948242 ms\n",
    "chunk:-> Hello!\n",
    "chunk:-> I am Mr.\n",
    "chunk:-> Ravi Ranjan’s assistant.\n",
    "chunk:-> How can I assist you today?\n",
    "chunk:-> Unfortunately, Mr.\n",
    "chunk:-> Ranjan is currently unavailable, but I can take your message and ensure he receives it.\n",
    "chunk:->  Please let me know your name and the purpose of your call!\n",
    "[DEBUG] LLM Response: Hello! I am Mr. Ravi Ranjan’s assistant. How can I assist you today? Unfortunately, Mr. Ranjan is currently unavailable, but I can take your message and ensure he receives it. Please let me know your name and the purpose of your call!\n",
    "```\n",
    "\n",
    "**Testing Outcome:**  \n",
    "The function handled streaming more effectively, but challenges persisted in managing complex responses with function calls.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Adjustments\n",
    "To handle function calls in streamed responses, I refined the logic further. The final implementation extracts:\n",
    "1. `function_name` for identifying the called function.\n",
    "2. `function_arguments` for capturing the arguments passed to the function.\n",
    "\n",
    "<!-- Placeholder for final refined function with function call handling -->\n",
    "*(Insert function here if needed to handle function calls in streaming responses.)*\n",
    "**Code**:\n",
    "```python\n",
    "def process_chunk(request):\n",
    "    try:\n",
    "        # Append user message to the messages array\n",
    "        if request:\n",
    "            messages.append({\"role\": \"user\", \"content\": request})\n",
    "\n",
    "        # Create the chat completion request with streaming\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        # Initialize variables for processing the response\n",
    "        complete_string = \"\"\n",
    "        continious_string = \"\"\n",
    "        chunk_timestamp = time.time()\n",
    "\n",
    "        # Process the streaming response\n",
    "        for chunk in response:\n",
    "            print(f\"chunk: {chunk}\")\n",
    "            current_gpt_chunk = chunk.choices[0].delta.content if chunk.choices else \"\"\n",
    "\n",
    "            if current_gpt_chunk and chunk.choices and chunk.choices[0].delta:\n",
    "                # Determine break punctuation\n",
    "                break_punctuation = [',', '!', '.', '?']\n",
    "                \n",
    "                if any(punc in current_gpt_chunk for punc in break_punctuation):\n",
    "                    continious_string += current_gpt_chunk\n",
    "                    complete_string += current_gpt_chunk\n",
    "\n",
    "                    # Print the chunk for debugging\n",
    "                    print(\"chunk:->\", continious_string)\n",
    "                    \n",
    "                    continious_string = \"\"\n",
    "                else:\n",
    "                    continious_string += current_gpt_chunk\n",
    "            if chunk.choices and chunk.choices[0].delta.tool_calls:\n",
    "                function_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                function_arguments = chunk.choices[0].delta.tool_calls[0].function.arguments\n",
    "                print(f\"function_name: {function_name}\")\n",
    "                print(f\"function_arguments: {function_arguments}\")        \n",
    "\n",
    "        return complete_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while processing the chunk: {e}\")\n",
    "        return None\n",
    "\n",
    "```\n",
    "\n",
    "**Key Refinements:**\n",
    "- Added handling for `tool_calls` in the streamed responses.\n",
    "- Improved error management for edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Results and Future Work\n",
    "**Results:**  \n",
    "- Streaming implementation successfully reduced initial response times.\n",
    "- The refined logic handled function calls and complex responses effectively.\n",
    "\n",
    "**Future Work:**\n",
    "- Optimize response assembly for higher performance.\n",
    "- Explore further improvements in tool integration to enhance LLM interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
