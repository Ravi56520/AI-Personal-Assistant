{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This documentation covers the components of a Python application that integrates Azure Cognitive Services for speech recognition, Twilio for messaging, and OpenAI for natural language processing. The application serves as a smart personal assistant for managing calls. The llm_module.py file is responsible for managing the interaction with the language model, processing user input, and generating appropriate responses. The azure_tts.py module handles text-to-speech conversion by communicating with Azure's API to generate audio from text and encoding the result in Base64 format for playback or further processing.\n",
    "\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "- `config.py`: Contains configuration settings and utility functions.\n",
    "- `audio_server.py`: Handles audio playback.\n",
    "- `azure_service_asr.py`: Manages speech recognition and interaction with the language model.\n",
    "- `llm_module.py`: Handles interactions with the OpenAI language model and processes user requests.\n",
    "- `azure_tts.py`: Converts text into speech using Azure Text-to-Speech services.\n",
    "- `utils.py`: Provides helper functions for communication and message management.\n",
    "\n",
    "## Configuration (`config.py`)\n",
    "\n",
    "## Module Overview \n",
    "This module contains configuration settings and utility functions that are essential for the application. It typically includes API keys, service endpoints, and other environment-specific variables to streamline the integration of various components\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "The following environment variables are loaded from a `.env` file:\n",
    "\n",
    "- `AZURE_TTS_URL`: URL for Azure Text-to-Speech service.\n",
    "- `AZURE_SUB_KEY`: Subscription key for Azure services.\n",
    "- `OPENAI_API_KEY`: API key for OpenAI services.\n",
    "- `TWILIO_ACCOUNT_SID`: Twilio account SID for messaging.\n",
    "- `TWILIO_AUTH_TOKEN`: Authentication token for Twilio.\n",
    "- `WHATSAPP_NUMBER`: WhatsApp number for sending messages.\n",
    "- `RECIPIENT_NUMBER`: Recipient's number for WhatsApp messages.\n",
    "- `AZURE_API_KEY`: API key for Azure services.\n",
    "- `AZURE_REGION`: Region for Azure services.\n",
    "\n",
    "### Global Variables\n",
    "\n",
    "- `client`: Instance of OpenAI client.\n",
    "- `recipient_name`: Placeholder for the recipient's name (default: \"Mr. Ravi Ranjan\").\n",
    "- `call_state`: Tracks the current call state (default: \"LISTENING\").\n",
    "- `recognizer`: Holds the speech recognizer instance.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `change_call_state(new_call_state)`\n",
    "\n",
    "Changes the global call state to the provided new state.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `new_call_state` (str): The new state to set (e.g., \"LISTENING\", \"SPEAKING\").\n",
    "  \n",
    "- **Returns**: \n",
    "  - (str): The updated call state.\n",
    "  \n",
    "- **Behavior**: This function updates the global `call_state` variable and returns the new state. It is used to control the flow of the assistant's operations based on the current interaction phase (e.g., listening or speaking).\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def change_call_state(new_call_state):\n",
    "    \"\"\"Change the global call state to the new state provided.\"\"\"\n",
    "    global call_state\n",
    "    call_state=new_call_state\n",
    "    return call_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_call_state()`\n",
    "\n",
    "Retrieves the current call state.\n",
    "\n",
    "- **Returns**:\n",
    "  - (str): The current call state.\n",
    "  \n",
    "- **Behavior**: This function simply returns the current value of the `call_state` variable, allowing other components of the application to determine if the assistant is currently listening or speaking.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def get_call_state():\n",
    "    \"\"\"Retrieve the current call state.\"\"\"\n",
    "    return call_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `change_recognizer(new_recognizer)`\n",
    "\n",
    "Updates the global recognizer with a new recognizer instance.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `new_recognizer`: The new recognizer instance to set.\n",
    "  \n",
    "- **Returns**:\n",
    "  - The updated recognizer instance.\n",
    "  \n",
    "- **Behavior**: This function changes the global `recognizer` variable to a new instance, which is essential when initializing or switching recognizers.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def change_recognizer(new_recognizer):\n",
    "    \"\"\"Update the global recognizer with a new recognizer instance.\"\"\"\n",
    "    global recognizer\n",
    "    recognizer=new_recognizer\n",
    "    return recognizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### `get_recognizer()`\n",
    "\n",
    "Gets the current recognizer instance.\n",
    "\n",
    "- **Returns**:\n",
    "  - The current recognizer instance.\n",
    "  \n",
    "- **Behavior**: Returns the instance of the recognizer that is currently set, allowing other parts of the code to access the speech recognition functionality.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def get_recognizer():\n",
    "    \"\"\"Get the current recognizer instance.\"\"\"\n",
    "    return recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "- **send_message_to_whatsapp**: Tool for sending messages via WhatsApp.\n",
    "  \n",
    "  - **Parameters**:\n",
    "    - `message` (str): The summary message to be sent to the recipient.\n",
    "    \n",
    "  - **Returns**: \n",
    "    - None.\n",
    "    \n",
    "  - **Behavior**: This tool is configured to send a message to the recipient via WhatsApp, using the parameters specified.\n",
    "**Code**:\n",
    "```python\n",
    "tools=[\n",
    "    {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"send_message_to_whatsapp\",\n",
    "        \"description\": \"This tool sends a message to Mr. Ravi Ranjan via WhatsApp.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"message\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The summary message to be sent to Mr. Ravi Ranjan.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"message\"]\n",
    "        }\n",
    "    }\n",
    "   }\n",
    "]\n",
    "```\n",
    "---\n",
    "\n",
    "## Audio Service (`audio_service.py`)\n",
    "\n",
    "## Module Overview \n",
    "This module is responsible for handling audio playback within the application. It manages audio streams, controls playback options, and ensures smooth delivery of sound during interactions, such as speech recognition or feedback.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `play_audio(audio_base64)`\n",
    "\n",
    "Plays audio from a base64-encoded string. Manages call state for automatic speech recognition (ASR) during playback.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `audio_base64` (str): A base64-encoded string representing the audio data.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: \n",
    "  - This function decodes the base64-encoded audio string, writes it to a temporary audio file (`output.mp3`), and plays the audio using the `playsound` library. \n",
    "  - It changes the `call_state` to \"SPEAKING\" before playback to pause ASR and reverts it back to \"LISTENING\" afterward, allowing for a seamless user experience.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def play_audio(audio_base64):\n",
    "    \"\"\"\n",
    "    Play audio from a base64-encoded string.\n",
    "\n",
    "    This function decodes a base64-encoded audio string, writes it to an \n",
    "    output file, and plays the audio using the playsound library. \n",
    "    It also manages the call state for automatic speech recognition (ASR) \n",
    "    by changing the state to \"SPEAKING\" before playback and reverting \n",
    "    it to \"LISTENING\" afterward.\n",
    "\n",
    "    Args:\n",
    "        audio_base64 (str): A base64-encoded string representing the audio data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    audio_data = base64.b64decode(audio_base64)\n",
    "    audio_file_path = \"output.mp3\"  \n",
    "\n",
    "    with open(audio_file_path, \"wb\") as audio_file:\n",
    "        audio_file.write(audio_data)\n",
    "    \n",
    "    # Stop ASR before playing audio\n",
    "    change_call_state(\"SPEAKING\")\n",
    "    print(\"[DEBUG] Playing audio and pausing ASR...\")\n",
    "    playsound(audio_file_path)  # This will play the audio file\n",
    "\n",
    "    change_call_state(\"LISTENING\")\n",
    "    print(\"[DEBUG] Audio playback finished. Restarting ASR...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Azure Service for ASR (`azure_service_asr.py`)\n",
    "\n",
    "## Module Overview\n",
    "This module manages the speech recognition functionality by interfacing with Azure's Automatic Speech Recognition (ASR) services. It handles audio input, processes speech recognition requests, and ensures accurate transcription of spoken language into text.\n",
    "### Global Variables\n",
    "\n",
    "- `call_state`: Initialized with the initial state.\n",
    "- `messages`: Holds message array for conversation history.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `initialize_speech_recognizer()`\n",
    "\n",
    "Initializes the Azure speech recognizer with the provided configuration.\n",
    "\n",
    "- **Returns**:\n",
    "  - `recognizer`: Instance of the speech recognizer.\n",
    "  \n",
    "- **Behavior**: This function creates and configures an instance of the Azure Speech Recognizer, setting it up with the necessary API keys and region information for Azure services.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def initialize_speech_recognizer():\n",
    "    \"\"\"Initialize the Azure speech recognizer.\"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_API_KEY, region=AZURE_REGION)\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    return recognizer\n",
    "```\n",
    "#### `start_recognition_if_listening(recognizer)`\n",
    "\n",
    "Starts continuous speech recognition if the current call state is \"LISTENING\".\n",
    "\n",
    "- **Parameters**:\n",
    "  - `recognizer`: The recognizer instance to start.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: This function checks the current `call_state`. If it is \"LISTENING\", it invokes the `start_continuous_recognition()` method on the recognizer, allowing it to begin processing audio input from the microphone.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def start_recognition_if_listening(recognizer):\n",
    "    \"\"\"Start continuous recognition if the call state is LISTENING.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    if call_state == \"LISTENING\":\n",
    "        recognizer.start_continuous_recognition()\n",
    "        print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ASR is not in the LISTENING state.\")\n",
    "```\n",
    "\n",
    "\n",
    "#### `process_recognized_text(recognized_text, recognizer)`\n",
    "\n",
    "Processes the recognized text and interacts with the language model.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `recognized_text` (str): The text recognized by the speech recognizer.\n",
    "  - `recognizer`: The recognizer instance used for processing.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: This function stops the speech recognition temporarily while processing the recognized text. It interacts with the language model (LLM) to generate responses based on the input, ensuring that the assistant can handle user queries effectively.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def process_recognized_text(recognized_text, recognizer):\n",
    "    \"\"\"Process the recognized text and interact with the LLM.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    change_call_state(\"SPEAKING\")\n",
    "    recognizer.stop_continuous_recognition()  # Stop ASR during response processing\n",
    "    print(\"[DEBUG] ASR stopped for LLM response processing.\")\n",
    "\n",
    "```\n",
    "#### `log_llm_response(llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred)`\n",
    "\n",
    "Logs details of the response from the language model.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `llm_response`: The response from the language model.\n",
    "  - `function_name`: Name of the function called.\n",
    "  - `function_arguments`: Arguments used in the function call.\n",
    "  - `function_id`: ID of the function.\n",
    "  - `first_chunk_time`: Time taken for the first response chunk.\n",
    "  - `error_occurred`: Flag indicating if an error occurred.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: This function prints debug information about the LLM's response and related parameters to the console. This is useful for monitoring and troubleshooting the interaction with the language model.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def log_llm_response(llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred):\n",
    "    \"\"\"Log details of the LLM response.\"\"\"\n",
    "    print(f\"[DEBUG] LLM Response: {llm_response}\")\n",
    "    print(f\"[DEBUG] Function Name: {function_name}\")\n",
    "    print(f\"[DEBUG] Function Arguments: {function_arguments}\")\n",
    "    print(f\"[DEBUG] Function ID: {function_id}\")\n",
    "    print(f\"[DEBUG] First Chunk Time: {first_chunk_time} ms\")\n",
    "    print(f\"[DEBUG] Error Occurred: {error_occurred}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### `handle_llm_response(llm_response, function_name, function_arguments, function_id)`\n",
    "\n",
    "Handles the response from the language model appropriately, sending messages if necessary.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `llm_response`: The response from the language model.\n",
    "  - `function_name`: Name of the function to be called if applicable.\n",
    "  - `function_arguments`: Arguments for the function call.\n",
    "  - `function_id`: ID of the function being handled.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: This function processes the LLM's response. If the response contains a message, it appends it to the conversation history. If a function needs to be called (e.g., sending a message), it performs the function call and logs the outcome.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def handle_llm_response(llm_response, function_name, function_arguments, function_id):\n",
    "    \"\"\"Handle the LLM response appropriately.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    if llm_response:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "    elif function_name:\n",
    "        append_asst_msg(messages=messages, function_id=function_id, function_name=function_name,\n",
    "                        function_args=function_arguments)\n",
    "        print(f\"[DEBUG] Calling function '{function_name}' with arguments: {function_arguments}\")\n",
    "        # Call the WhatsApp sending function\n",
    "        function_returns = send_message_to_whatsapp(function_arguments)\n",
    "        append_tool_call_message(messages=messages, function_id=function_id, function_name=function_name,\n",
    "                                 function_returns=function_returns)\n",
    "        llm_response, _, _, _, _, _ = process_chunk(None, client, messages, tools)\n",
    "        print(f\"LLM Response after function call: {llm_response}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "    \n",
    "    change_call_state(\"LISTENING\")\n",
    "```\n",
    "\n",
    "#### `handle_recognition_result(evt, recognizer)`\n",
    "\n",
    "Handles the result of the speech recognition event.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `evt`: The event object containing recognition details.\n",
    "  - `recognizer`: The recognizer instance handling the event.\n",
    "  \n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: This function responds to recognition events by checking the result's reason (e.g., recognized speech, no match, canceled). If speech was recognized, it processes the recognized text. It also logs the results of the interaction with the language model and continues listening for further input.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def handle_recognition_result(evt, recognizer):\n",
    "    \"\"\"Handle the recognition result from the speech recognizer.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "\n",
    "    if call_state != \"LISTENING\":\n",
    "        print(\"[DEBUG] ASR ignored input since call_state is not LISTENING.\")\n",
    "        return\n",
    "\n",
    "    print(\"[DEBUG] LISTENING AGAIN\")\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        recognized_text = evt.result.text.strip()\n",
    "        if recognized_text:\n",
    "            print(f\"[DEBUG] ASR recognized text: {recognized_text}\")\n",
    "            process_recognized_text(recognized_text, recognizer)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"[DEBUG] No speech could be recognized.\")\n",
    "    elif evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "        print(f\"[DEBUG] Recognition canceled: {evt.result.cancellation_details.reason}\")\n",
    "\n",
    "    # Process the recognized text with process_chunk\n",
    "    llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred = process_chunk(recognized_text, client, messages, tools)\n",
    "    \n",
    "    # Log the results\n",
    "    log_llm_response(llm_response, function_name, function_arguments, function_id, first_chunk_time, error_occurred)\n",
    "\n",
    "    # Handle the LLM response\n",
    "    handle_llm_response(llm_response, function_name, function_arguments, function_id)\n",
    "    recognizer.start_continuous_recognition()\n",
    "        print(f\"[DEBUG] Recognition canceled: {evt.result.cancellation_details.reason}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### `recognize_speech_continuously()`\n",
    "\n",
    "Continuously recognizes speech using Azure Speech Service.\n",
    "\n",
    "- **Returns**:\n",
    "  - None.\n",
    "  \n",
    "- **Behavior**: \n",
    "  - This function initializes the speech recognizer, sets up the event listeners for recognition events, and starts the recognition process. \n",
    "  - It maintains an ongoing loop, allowing the system to keep listening for speech input until the `call_state` is set to \"STOP\", at which point it safely terminates recognition.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def recognize_speech_continuously():\n",
    "    \"\"\"Continuously recognize speech using Azure Speech Service.\"\"\"\n",
    "    global call_state  # Access the global call_state variable\n",
    "    call_state = get_call_state()  # Refresh call_state if needed\n",
    "    \n",
    "    recognizer = initialize_speech_recognizer()\n",
    "    change_recognizer(recognizer)  # Ensure the recognizer is set globally\n",
    "    print(\"[DEBUG] ASR started and is now listening...\")\n",
    "    print(\"[DEBUG] ASR initialized. Waiting to start recognition...\")\n",
    "    \n",
    "    initiate_conversation_with_llm()\n",
    "    print(\"Listening... Speak into your microphone.\")\n",
    "    \n",
    "    recognizer.recognized.connect(lambda evt: handle_recognition_result(evt, recognizer))\n",
    "    \n",
    "    # Start ASR only if it's in the correct state\n",
    "    start_recognition_if_listening(recognizer)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            call_state = get_call_state()\n",
    "            if call_state == \"STOP\":\n",
    "                recognizer.stop_continuous_recognition()\n",
    "                print(\"[DEBUG] ASR is now STOPPING\")\n",
    "                break \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[DEBUG] Stopping recognition...\")\n",
    "        print(f\"traceback in recognize_speech_continuously: {traceback.format_exc()}\")\n",
    "        recognizer.stop_continuous_recognition()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Module (`llm_module.py`)\n",
    "\n",
    "## Module Overview:\n",
    "This module handles interactions with the OpenAI language model, processing user requests, and generating responses. It serves as the bridge between user input and the language model's capabilities, enabling dynamic conversation and content generation.\n",
    "\n",
    "### Global Variables\n",
    "\n",
    "- `messages`: List of messages to maintain the conversation context, initialized with a system prompt.\n",
    "- `call_state`: Current state of the call, retrieved using `get_call_state()`.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `get_message_array()`\n",
    "\n",
    "Returns the current array of messages.\n",
    "\n",
    "- **Returns**:\n",
    "  - The current array of messages (list).\n",
    "\n",
    "- **Behavior**: This function provides access to the conversation history, allowing other components to review the messages exchanged with the user.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def get_message_array():\n",
    "    \"\"\"Returns the current array of messages.\"\"\"\n",
    "    return messages\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### `append_user_message(messages, request)`\n",
    "\n",
    "Appends a user's message to the messages array if the request is not empty.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `messages`: The current array of messages (list).\n",
    "  - `request` (str): The user's message to append.\n",
    "\n",
    "- **Returns**: \n",
    "  - None.\n",
    "\n",
    "- **Behavior**: This function checks if the user's request is not empty before appending it to the messages array, maintaining a record of user interactions.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def append_user_message(messages, request):\n",
    "    \"\"\"Appends a user's message to the messages array if the request is not empty.\"\"\"\n",
    "    if request:\n",
    "        messages.append({\"role\": \"user\", \"content\": request})\n",
    "```\n",
    "\n",
    "\n",
    "#### `create_chat_completion(client, messages, tools)`\n",
    "\n",
    "Creates a chat completion using the specified client and messages.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `client`: The OpenAI client instance used for communication.\n",
    "  - `messages`: The array of messages forming the conversation context (list).\n",
    "  - `tools`: The available tools for processing (list).\n",
    "\n",
    "- **Returns**:\n",
    "  - The chat completion response object.\n",
    "\n",
    "- **Behavior**: This function generates a response from the OpenAI model based on the current messages and tools, enabling dynamic interactions with the user.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def create_chat_completion(client, messages, tools):\n",
    "    \"\"\"Creates a chat completion using the specified client and messages.\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=True\n",
    "    )\n",
    "```\n",
    "---\n",
    "\n",
    "#### `determine_break_punctuation(count)`\n",
    "\n",
    "Returns punctuation marks used for breaking continuous speech based on the count of processed chunks.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `count` (int): The number of processed chunks.\n",
    "\n",
    "- **Returns**:\n",
    "  - A list of punctuation marks (list).\n",
    "\n",
    "- **Behavior**: This function determines which punctuation marks are appropriate for splitting the text based on the number of chunks, ensuring coherent audio playback.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "def determine_break_punctuation(count):\n",
    "    \"\"\"Returns punctuation marks used for breaking continuous speech based on the count of processed chunks.\"\"\"\n",
    "    if count <= 2:\n",
    "        return [',', '!', ':', '.', '?', '|', '।', '፧', '፨', '،', '؛', '؟']\n",
    "    else:\n",
    "        return ['.', '?', '।', '፧', '፨', '؛', '؟']\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### `check_punctuation_split(continious_string, current_gpt_chunk)`\n",
    "\n",
    "Checks if the continuous string can be split based on punctuation rules.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `continious_string` (str): The ongoing text that is being processed.\n",
    "  - `current_gpt_chunk` (str): The latest chunk of text received.\n",
    "\n",
    "- **Returns**:\n",
    "  - A tuple containing:\n",
    "    - A boolean indicating if the string can be split (bool).\n",
    "    - The modified continuous string (str).\n",
    "    - Any discarded string that was not included in the split (str).\n",
    "\n",
    "- **Behavior**: This function assesses whether the ongoing string can be split based on specific punctuation rules and conditions, ensuring proper text processing.\n",
    "**Code**:\n",
    "```python\n",
    "def check_punctuation_split(continious_string, current_gpt_chunk):\n",
    "    \"\"\"Checks if the continuous string can be split based on punctuation rules.\"\"\"\n",
    "    words = continious_string.split()\n",
    "    if not words:\n",
    "        return False, \"\", \"\"\n",
    "\n",
    "    last_word = words[-1]\n",
    "    if last_word in [\"Mr.\", \"Dr.\", \"Ms.\", \"Mrs.\"]:\n",
    "        return False, continious_string, \"\"\n",
    "\n",
    "    discarded_string = \"\"\n",
    "    if current_gpt_chunk in [\",\", \".\"]:\n",
    "        if check_before_or_after_comma_is_number(last_word):\n",
    "            discarded_string = \" \" + last_word\n",
    "            continious_string = ' '.join(words[:-1])\n",
    "    \n",
    "    return True, continious_string, discarded_string\n",
    "```\n",
    "---\n",
    "\n",
    "#### `generate_and_play_audio(text)`\n",
    "\n",
    "Generates audio from text and plays it if the text is not empty.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `text` (str): The text to convert to audio.\n",
    "\n",
    "- **Returns**:\n",
    "  - None.\n",
    "\n",
    "- **Behavior**: This function generates audio from the given text and plays it using the audio service if the text is not empty, ensuring effective audio feedback to the user.\n",
    "**Code**:\n",
    "```python\n",
    "def generate_and_play_audio(text):\n",
    "    \"\"\"Generates audio from text and plays it if the text is not empty.\"\"\"\n",
    "    if text:\n",
    "        print(\"chunk:->\", text)\n",
    "        audio_base64 = generate_audio_azure(text)\n",
    "        if audio_base64:\n",
    "            play_audio(audio_base64)  # Implement play_audio for playback\n",
    "        else:\n",
    "            print(\"[ERROR] Failed to generate audio.\")\n",
    "```\n",
    "---\n",
    "\n",
    "#### `extract_function_calls(chunk)`\n",
    "\n",
    "Extracts function calls from a chunk of response data.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `chunk`: The response chunk from the language model.\n",
    "\n",
    "- **Returns**:\n",
    "  - A tuple containing:\n",
    "    - The extracted function name (str).\n",
    "    - The function arguments (str).\n",
    "    - The function ID (str).\n",
    "\n",
    "- **Behavior**: This function analyzes the response chunk for any function calls, allowing the system to invoke the necessary tools or functions based on the model's output.\n",
    "**Code**:\n",
    "```python\n",
    "def extract_function_calls(chunk):\n",
    "    \"\"\"Extracts function calls from a chunk of response data.\"\"\"\n",
    "    function_name = None\n",
    "    function_id = None\n",
    "    function_arguments = ''\n",
    "    \n",
    "    if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.tool_calls:\n",
    "        tool_call = chunk.choices[0].delta.tool_calls[0]\n",
    "        if tool_call.function:\n",
    "            function_name = tool_call.function.name\n",
    "            function_id = tool_call.id\n",
    "            print(f\"function_name: {function_name}\")\n",
    "            if tool_call.function.arguments:\n",
    "                function_arguments += tool_call.function.arguments\n",
    "                print(f\"function_arguments :{function_arguments}\")\n",
    "    \n",
    "    return function_name, function_arguments, function_id\n",
    "```\n",
    "---\n",
    "\n",
    "#### `process_streaming_response(response, initial_timestamp)`\n",
    "\n",
    "Processes the streaming response from the language model, accumulating generated text and managing audio playback.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `response`: The streaming response object containing generated text chunks.\n",
    "  - `initial_timestamp`: The timestamp when the streaming started.\n",
    "\n",
    "- **Returns**:\n",
    "  - A tuple with the complete response string (str), function name (str), function arguments (str), function ID (str), and time for the first chunk (int).\n",
    "\n",
    "- **Behavior**: This function accumulates generated text from the model, detects punctuation for audio playback, and manages the timing of audio generation, providing a seamless experience.\n",
    "**Code**:\n",
    "```python\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def process_streaming_response(response, initial_timestamp):\n",
    "    \"\"\"\n",
    "    Processes the streaming response from the language model, accumulating generated text and managing audio playback.\n",
    "\n",
    "    It tracks the time taken for the first chunk, detects function calls, and generates audio output as needed.\n",
    "\n",
    "    Args:\n",
    "        response: The streaming response object containing generated text chunks.\n",
    "        initial_timestamp: The timestamp when the streaming started.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with the complete response string, function name, function arguments, function ID, and time for the first chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    complete_string = \"\"\n",
    "    continious_string = \"\"\n",
    "    first_chunk_time = 0\n",
    "    count = 1\n",
    "    function_id = None\n",
    "    function_name = None\n",
    "    function_arguments = ''\n",
    "    chunk_timestamp = initial_timestamp\n",
    "    \n",
    "    for chunk in response:\n",
    "        current_gpt_chunk = chunk.choices[0].delta.content if chunk.choices else \"\"\n",
    "        \n",
    "        if current_gpt_chunk and chunk.choices and chunk.choices[0].delta:\n",
    "            break_punctuation = determine_break_punctuation(count)\n",
    "            \n",
    "            if any(punc in current_gpt_chunk for punc in break_punctuation):\n",
    "                continious_string += current_gpt_chunk\n",
    "                complete_string += current_gpt_chunk\n",
    "\n",
    "                if count == 1:\n",
    "                    first_chunk_time = (time.time() - chunk_timestamp) * 1000\n",
    "                    print(f\"[DEBUG] Time taken to generate 1st chunk: {first_chunk_time} ms\")\n",
    "\n",
    "                should_split, continious_string, discarded_string = check_punctuation_split(continious_string, current_gpt_chunk)\n",
    "                if should_split:\n",
    "                    chunk_timestamp = time.time()\n",
    "                    continious_string = continious_string.strip()\n",
    "                    generate_and_play_audio(continious_string)\n",
    "                    continious_string = discarded_string\n",
    "                    count += 1\n",
    "            else:\n",
    "                continious_string += current_gpt_chunk\n",
    "                complete_string += current_gpt_chunk\n",
    "\n",
    "        # Extract function calls if any\n",
    "        fn_name, fn_args, fn_id = extract_function_calls(chunk)\n",
    "        if fn_name:\n",
    "            function_name = fn_name\n",
    "        if fn_args:\n",
    "            function_arguments += fn_args\n",
    "        if fn_id:\n",
    "            function_id = fn_id\n",
    "\n",
    "    if continious_string:\n",
    "            print(\"chunk:->\", continious_string)\n",
    "            audio_base64 = generate_audio_azure(continious_string)\n",
    "\n",
    "            if audio_base64:\n",
    "                play_audio(audio_base64) \n",
    "            else:\n",
    "                print(\"[ERROR] Failed to generate final audio.\")\n",
    "\n",
    "    return complete_string, function_name, function_arguments, function_id, first_chunk_time\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def process_chunk(request, client, messages, tools):\n",
    "    \"\"\"\n",
    "    Handles a user request by appending the message and generating a response from the chat client.\n",
    "\n",
    "    Captures exceptions during processing and manages the overall flow of information.\n",
    "\n",
    "    Args:\n",
    "        request: The user's message to process.\n",
    "        client: The chat client for interaction with the language model.\n",
    "        messages: The conversation context.\n",
    "        tools: The available tools for processing.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the complete response string, function name, function arguments, function ID, first chunk time, and an error flag.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        append_user_message(messages, request)\n",
    "        response = create_chat_completion(client, messages, tools)\n",
    "        \n",
    "        initial_timestamp = time.time()\n",
    "        (complete_string, function_name, \n",
    "         function_arguments, function_id, \n",
    "         first_chunk_time) = process_streaming_response(response, initial_timestamp)\n",
    "        \n",
    "        return complete_string, function_name, function_arguments, function_id, first_chunk_time, False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while processing the chunk: {e}\")\n",
    "        print(f\"traceback in process chunk : {traceback.format_exc()}\")\n",
    "        return None, None, None, None, None, True\n",
    "```\n",
    "---\n",
    "\n",
    "#### `process_chunk(request, client, messages, tools)`\n",
    "\n",
    "Handles a user request by appending the message and generating a response from the chat client.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `request`: The user's message to process (str).\n",
    "  - `client`: The chat client for interaction with the language model.\n",
    "  - `messages`: The conversation context (list).\n",
    "  - `tools`: The available tools for processing (list).\n",
    "\n",
    "- **Returns**:\n",
    "  - A tuple containing the complete response string (str), function name (str), function arguments (str), function ID (str), first chunk time (int), and an error flag (bool).\n",
    "\n",
    "- **Behavior**: This function captures exceptions during processing, manages the overall flow of information, and communicates with the OpenAI model to generate responses.\n",
    "**Code**:\n",
    "```python\n",
    "```\n",
    "---\n",
    "\n",
    "#### `initiate_conversation_with_llm()`\n",
    "\n",
    "Initiates a conversation with the language model by sending a greeting message to the caller.\n",
    "\n",
    "- **Returns**:\n",
    "  - None.\n",
    "\n",
    "- **Behavior**: This function sends an initial query to the language model, prompting it to introduce the assistant and ask for the caller's name and purpose. It appends the generated response to the conversation context.\n",
    "**Code**:\n",
    "```python\n",
    "def initiate_conversation_with_llm():\n",
    "    \"\"\"\n",
    "    Initiates a conversation with the language model by sending a greeting message to the caller.\n",
    "\n",
    "    Sends an initial query to generate an introductory message and appends the response to the conversation context.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    global messages\n",
    "    if call_state == \"LISTENING\":\n",
    "        initial_query = \"Please introduce yourself to the caller as Mr. Ravi Ranjan’s assistant and ask for their name and the purpose of their call.\"\n",
    "        print(\"[DEBUG] Sending initial query to LLM...\")\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                stream= True\n",
    "            )\n",
    "            llm_response, _, _, _, _, _ = process_chunk(initial_query,client,messages,tools)\n",
    "            messages.append({\"role\":\"assistant\",\"content\":llm_response})\n",
    "\n",
    "            print(f\"LLM Generated Greeting: {llm_response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] An error occurred while initiating the conversation: {e}\")\n",
    "            print(f\"traceback in intitial  chunk : {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure TTS Module (`azure_tts.py`)\n",
    "\n",
    "## Module Overview\n",
    "This module is responsible for converting text into speech using Azure Text-to-Speech (TTS) services. It takes generated text responses and synthesizes them into natural-sounding audio output for effective communication with users.\n",
    "\n",
    "### Function\n",
    "\n",
    "#### `generate_audio_azure(text, lang=\"en-US\", voice_name=\"en-US-AvaNeural\", tts_style=\"chat\", retry=True)`\n",
    "\n",
    "Converts text into audio using the Azure Text-to-Speech API.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `text` (str): The text to convert into speech.\n",
    "  - `lang` (str, optional): The language code for the speech synthesis (default is `\"en-US\"`).\n",
    "  - `voice_name` (str, optional): The voice name to use for speech synthesis (default is `\"en-US-AvaNeural\"`).\n",
    "  - `tts_style` (str, optional): The style of speech (default is `\"chat\"`).\n",
    "  - `retry` (bool, optional): A flag to retry the request in case of failure (default is `True`).\n",
    "\n",
    "- **Returns**:\n",
    "  - A Base64-encoded audio string (str) if the request is successful.\n",
    "  - An empty string (`\"\"`) if the request fails.\n",
    "\n",
    "- **Behavior**:\n",
    "  1. The function sanitizes the input text by replacing problematic characters such as `&`, `<`, and `>` to ensure compatibility with Azure TTS's XML format.\n",
    "  2. It constructs an SSML (Speech Synthesis Markup Language) payload with the specified language, voice, and style.\n",
    "  3. Sends a POST request to Azure TTS API using the provided configuration.\n",
    "  4. If successful, it encodes the audio response into Base64 format and returns it.\n",
    "  5. Handles errors gracefully, printing error messages and returning an empty string in case of failure.\n",
    "\n",
    "- **Headers**:\n",
    "  - `Ocp-Apim-Subscription-Key`: The subscription key for Azure API authentication.\n",
    "  - `Content-Type`: Specifies that the request content is SSML.\n",
    "  - `X-Microsoft-OutputFormat`: Sets the audio output format (e.g., 16 kHz mono MP3).\n",
    "  - `User-Agent`: Identifies the client making the request.\n",
    "\n",
    "- **Request Data**:\n",
    "  - The function generates an SSML payload containing the text, language, and voice settings for the speech synthesis.\n",
    "\n",
    "- **Error Handling**:\n",
    "  - If the API returns a non-200 status code, the function raises an exception.\n",
    "  - Any other errors during the request or processing are caught and logged.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "from azure_tts import generate_audio_azure\n",
    "\n",
    "# Example text to convert to audio\n",
    "text_to_convert = \"Hello, this is an example of text-to-speech conversion.\"\n",
    "\n",
    "# Generate audio in Base64 format\n",
    "audio_base64 = generate_audio_azure(text_to_convert)\n",
    "\n",
    "if audio_base64:\n",
    "    print(\"Audio successfully generated!\")\n",
    "else:\n",
    "    print(\"Failed to generate audio.\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Module (`utils.py`)\n",
    "\n",
    "## Module Overview\n",
    "This module provides a collection of helper functions that support various tasks within the application, including communication between components, message management, and utility functions that enhance overall functionality and maintainability.\n",
    "\n",
    "### Functions\n",
    "\n",
    "#### `send_message_to_whatsapp(message)`\n",
    "\n",
    "Sends a WhatsApp message using Twilio's API.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `message` (str): The content of the message to be sent.\n",
    "\n",
    "- **Returns**:\n",
    "  - A success message (`\"Message sent successfully\"`) if the operation is successful.\n",
    "  - An error message (`\"Error in sending message.\"`) if the operation fails.\n",
    "\n",
    "- **Behavior**:\n",
    "  1. Authenticates with Twilio using credentials from `config.py`.\n",
    "  2. Attempts to send a WhatsApp message to the recipient number.\n",
    "  3. Prints the message SID if successful or logs the error if it fails.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  def send_message_to_whatsapp(message):\n",
    "    client = Client(ACCOUNT_SID, AUTH_TOKEN)\n",
    "    try:\n",
    "        msg = client.messages.create(\n",
    "            from_=WHATSAPP_NUMBER,\n",
    "            body=message,\n",
    "            to=f'whatsapp:{RECIPIENT_NUMBER}'\n",
    "        )\n",
    "        print(f\"[INFO] WhatsApp message sent. SID: {msg.sid}\")\n",
    "        return \"Message sent successfully\"\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to send WhatsApp message: {e}\")\n",
    "        return \"Error in sending message.\n",
    "\n",
    "  message = \"Hello from Twilio!\"\n",
    "  result = send_message_to_whatsapp(message)\n",
    "  print(result)\n",
    "  ```\n",
    "  \n",
    "### `check_before_or_after_comma_is_number(s)`\n",
    "\n",
    "**Description**:  \n",
    "This function checks whether a number is located immediately before or after a comma (`,`), or a period (`.`) in the given string.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `s` (str): The input string to validate.\n",
    "\n",
    "- **Returns**:\n",
    "  - `True` if a number is found before or after a comma/period.\n",
    "  - `False` otherwise.\n",
    "\n",
    "- **Behavior**:\n",
    "  - Uses the regular expression `r'\\d[,.]|[,.]\\d'` to detect patterns such as:\n",
    "    - `1,000`\n",
    "    - `3.14`\n",
    "    - `,1`\n",
    "  - Handles exceptions gracefully by logging an error message and returning `False`.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "\n",
    "  text_1 = \"The price is 1,000 USD.\"\n",
    "  text_2 = \"Hello, world!\"\n",
    "  result_1 = check_before_or_after_comma_is_number(text_1)  # Output: True\n",
    "  result_2 = check_before_or_after_comma_is_number(text_2)  # Output: False\n",
    "  ```\n",
    "\n",
    "### `append_asst_msg(messages, function_id, function_name, function_args)`\n",
    "\n",
    "**Description**:  \n",
    "This function appends a message to the `messages` list, representing a response from an assistant. The message includes metadata about a tool or function the assistant has invoked.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `messages` (list): The conversation history stored as a list of message objects.\n",
    "  - `function_id` (str): A unique identifier for the invoked function or tool call.\n",
    "  - `function_name` (str): The name of the function/tool invoked by the assistant.\n",
    "  - `function_args` (str): The arguments passed to the function/tool.\n",
    "\n",
    "- **Returns**:  \n",
    "  - None (modifies the `messages` list in place).\n",
    "\n",
    "- **Behavior**:  \n",
    "  - Appends an object to the `messages` list with the following structure:\n",
    "    ```json\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": null,\n",
    "      \"tool_calls\": [\n",
    "        {\n",
    "          \"id\": \"function_id\",\n",
    "          \"function\": {\n",
    "            \"name\": \"function_name\",\n",
    "            \"arguments\": \"function_args\"\n",
    "          },\n",
    "          \"type\": \"function\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  conversation = []\n",
    "  append_asst_msg(conversation, \"tool_001\", \"fetch_data\", '{\"param1\": \"value1\"}')\n",
    "  print(conversation)  \n",
    "  ```\n",
    "\n",
    "\n",
    "## Function: `append_tool_call_message`\n",
    "\n",
    "**Description**:  \n",
    "Appends a tool call message to the conversation history. This message captures the return value of a previously invoked tool or function.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`messages`** (`list`):  \n",
    "  The conversation history, represented as a list of message objects. This list is modified in place to include the new tool call message.\n",
    "\n",
    "- **`function_id`** (`str`):  \n",
    "  A unique identifier that links this message to the original tool or function call.\n",
    "\n",
    "- **`function_name`** (`str`):  \n",
    "  The name of the tool or function that generated the return value.\n",
    "\n",
    "- **`function_returns`** (`str`):  \n",
    "  The return value or output of the tool or function.\n",
    "\n",
    "---\n",
    "\n",
    "### Returns\n",
    "\n",
    "- **None**:  \n",
    "  The function modifies the `messages` list in place and does not return a value.\n",
    "\n",
    "---\n",
    "\n",
    "### Behavior\n",
    "\n",
    "The function appends a new dictionary to the `messages` list with the following structure:\n",
    "```json\n",
    "{\n",
    "  \"tool_call_id\": \"function_id\",\n",
    "  \"role\": \"tool\",\n",
    "  \"name\": \"function_name\",\n",
    "  \"content\": \"function_returns\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## frontend Module (`frontend.py`)\n",
    "\n",
    "## Overview\n",
    "This document describes the implementation of a phone call interface using Streamlit, which allows users to manage a phone call session with speech recognition capabilities. The interface includes buttons to start and end calls, a timer to track call duration, and a message display area to show conversation history.\n",
    "\n",
    "### Page Configuration\n",
    "- `st.set_page_config()`: Configures the page title, icon, and layout to be centered.\n",
    "\n",
    "### Custom Styles\n",
    "- `st.markdown()`: Applies custom CSS styles for the main container, status messages, and timer display to enhance the user interface.\n",
    "\n",
    "### Session State Initialization\n",
    "The following session states are initialized:\n",
    "- **`call_status`**: Indicates the current status of the call (e.g., \"No call in progress\").\n",
    "- **`call_start_time`**: Records the time when the call starts.\n",
    "- **`elapsed_time`**: Tracks the total duration of the call in seconds.\n",
    "- **`messages`**: A list to store chat messages during the call.\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "### `format_duration(seconds)`\n",
    "**Description**: Formats the call duration in hours, minutes, and seconds.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `seconds` (int): The total duration in seconds.\n",
    "  \n",
    "- **Returns**:\n",
    "  - (str): A string representing the formatted duration (HH:MM:SS).\n",
    "\n",
    "### `on_call_connected()`\n",
    "**Description**: Executed when the call is connected. It changes the call state to \"LISTENING\" and starts the speech recognition process.\n",
    "\n",
    "### `on_call_ended()`\n",
    "**Description**: Executed when the call ends. It changes the call state to \"STOP\" to perform cleanup actions.\n",
    "\n",
    "## Main Logic\n",
    "- **Call Timer Logic**: Calculates the elapsed time based on the call status.\n",
    "\n",
    "## User Interface Elements\n",
    "\n",
    "### Title and Main Container\n",
    "- `st.title()`: Displays the title of the application.\n",
    "- `st.markdown()`: Creates a styled main container for the UI.\n",
    "\n",
    "### Status Container\n",
    "- `set_markdown(status_message)`: A helper function to display the current call status with appropriate styling based on whether the call is ongoing or has ended.\n",
    "\n",
    "### Call Timer Display\n",
    "- Displays the formatted call duration when the call is active.\n",
    "\n",
    "## Call Control Buttons\n",
    "Two buttons are provided for managing call status:\n",
    "\n",
    "### 📞 Start Call:\n",
    "- Sets the call status to \"Call is started\".\n",
    "- Records the start time and updates the status message.\n",
    "- Calls `on_call_connected()` to initiate speech recognition.\n",
    "\n",
    "### 🔴 End Call:\n",
    "- Sets the call status to \"Call is ended\".\n",
    "- Updates the status message.\n",
    "- Calls `on_call_ended()` to stop any ongoing processes.\n",
    "\n",
    "## Message Display\n",
    "- Iterates through the stored messages in `st.session_state.messages` and displays them in the UI, indicating whether they are from the assistant or the user.\n",
    "\n",
    "## Footer\n",
    "- Adds a horizontal line to separate the footer from the main content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phone Call Interface](ui_image.jpeg)\n",
    "\n",
    "This image illustrates the layout of the phone call interface, showcasing the call status, timer, and message display area. The design emphasizes clarity and ease of use, ensuring a smooth user experience during phone call management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create readme to run above code"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
